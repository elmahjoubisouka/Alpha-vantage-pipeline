# ğŸ“ˆ Alpha Vantage Financial Data Pipeline

## ğŸ“‹ Vue d'ensemble

Ce projet implÃ©mente un pipeline Big Data financier complet pour la collecte, le traitement, l'enrichissement et la visualisation de donnÃ©es boursiÃ¨res Ã  partir de l'API Alpha Vantage.

Le pipeline est :
- **OrchestrÃ©** par Apache Airflow  
- **StockÃ©** dans Snowflake  
- **TraitÃ©** avec Spark SQL  
- **VisualisÃ©** via Streamlit  
- **DÃ©ployÃ©** avec Docker  

---

## ğŸ“Š Table des matiÃ¨res

- [Architecture](#-architecture-globale)
- [Ã‰tapes d'installation](#-Ã©tapes-dinstallation)
- [Configuration](#-configuration)
- [Utilisation](#-utilisation)
- [Visualisations](#-visualisations)
- [Structure du projet](#-structure-du-projet)
- [Technologies](#-technologies)

---

## ğŸ—ï¸ Architecture globale

![Architecture du projet](diagram-export-16-12-2025-15_56_25.png)

**Flux de donnÃ©es :**
```
Collecte â†’ Airflow DAG â†’ API Alpha Vantage â†’ Snowflake (brut) 
â†’ Spark SQL â†’ Indicateurs techniques â†’ Snowflake (enrichi) â†’ Streamlit Dashboard
```

---

## ğŸ“ Ã‰tapes d'installation

### 1. PrÃ©requis

- Python 3.11+
- Docker
- Compte Snowflake
- ClÃ© API Alpha Vantage

### 2. Cloner le projet
```bash
git clone https://github.com/elmahjoubisouka/Alpha-vantage-pipeline.git
cd airflow-finance-project
```

### 3. Installer les dÃ©pendances
```bash
pip install -r requirements.txt
```

### 4. Configuration Snowflake

ExÃ©cuter dans Snowflake :
```sql
USE DATABASE FINANCE_DB;
USE SCHEMA RAM_DATA;

CREATE OR REPLACE TABLE ALPHA_VANTAGE_DATA (
    symbol VARCHAR(10) NOT NULL,
    date DATE NOT NULL,
    open DECIMAL(10, 4),
    high DECIMAL(10, 4),
    low DECIMAL(10, 4),
    close DECIMAL(10, 4),
    volume BIGINT,
    dividends DECIMAL(10, 4),
    stock_splits DECIMAL(10, 4),
    batch_id VARCHAR(50),
    data_source VARCHAR(50),
    api_key_used VARCHAR(20),
    load_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),
    CONSTRAINT pk_alpha_vantage PRIMARY KEY (symbol, date, batch_id)
);
```

### 5. Variables d'environnement (.env)

CrÃ©er un fichier `.env` Ã  la racine du projet :
```bash
ALPHA_VANTAGE_API_KEY=votre_cle
SNOWFLAKE_ACCOUNT=votre_compte
SNOWFLAKE_USER=votre_user
SNOWFLAKE_PASSWORD=votre_mdp
SNOWFLAKE_DATABASE=FINANCE_DB
SNOWFLAKE_SCHEMA=RAM_DATA
```

### 6. Installer le connecteur Spark-Snowflake
```bash
mkdir -p /opt/airflow/spark_jars
curl -L -o /opt/airflow/spark_jars/spark-snowflake_2.12-2.11.0-spark_3.3.jar \
https://repo1.maven.org/maven2/net/snowflake/spark-snowflake_2.12/2.11.0-spark_3.3/spark-snowflake_2.12-2.11.0-spark_3.3.jar
```

### 7. Lancer avec Docker
```bash
docker-compose up -d
docker-compose ps
```

---

## ğŸš€ Utilisation

### 1. DÃ©marrer Airflow

- AccÃ©der Ã  l'interface web : [http://localhost:8081](http://localhost:8081)
- Activer le DAG `alpha_vantage_project`

### 2. Lancer Streamlit
```bash
python -m streamlit run streamlit_dashboard.py
```

- AccÃ©der au dashboard : [http://localhost:8501](http://localhost:8501)

---

## ğŸ“Š Visualisations

Le dashboard Streamlit offre :

- **Signaux de trading** (RSI, MACD)
- **Performance par pÃ©riode**
- **Matrice de corrÃ©lations**
- **DonnÃ©es brutes et enrichies**

---

## ğŸ“ Structure du projet
```
airflow-finance-project/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ alpha_vantage_project.py
â”œâ”€â”€ processors/
â”‚   â””â”€â”€ spark_processor.py
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ data_collection.py
â”œâ”€â”€ streamlit_dashboard.py
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ”§ Technologies

- **Apache Airflow** 2.8.0
- **Snowflake** (Data Warehouse)
- **Spark SQL** 3.3
- **Streamlit** 1.52.1
- **Alpha Vantage API**
- **Docker**
- **Python** 3.11


## ğŸ“§ Contact

Pour toute question ou suggestion, n'hÃ©sitez pas Ã  ouvrir une issue sur GitHub.
