"""
DAG: alpha_vantage_finance_pipeline
API Alpha Vantage avec cl√© : 3M7KBIE2THH3X52R
Base de donn√©es: FINANCE_DB (corrig√©)
√âquipe: Assia Boujnah, Soukaina El Mahjoubi, Khalil Fatima
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.snowflake.hooks.snowflake import SnowflakeHook
import pandas as pd
import requests
import logging
import time
from typing import List, Dict

logger = logging.getLogger(__name__)

# ============================================================================
# CONFIGURATION - CORRIG√âE POUR FINANCE_DB
# ============================================================================
default_args = {
    'owner': 'equipe_finance',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# VOTRE CL√â API ALPHA VANTAGE
ALPHA_VANTAGE_API_KEY = "3M7KBIE2THH3X52R"

# VOTRE BASE DE DONN√âES (corrig√©e)
SNOWFLAKE_DATABASE = "FINANCE_DB"
SNOWFLAKE_SCHEMA = "RAW_DATA"

# Symboles √† r√©cup√©rer
PROJECT_SYMBOLS = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'NVDA', 'META', 'JNJ']

# Batch ID unique
BATCH_ID = f"AV_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

# ============================================================================
# FONCTIONS DE R√âCUP√âRATION DES DONN√âES
# ============================================================================

def test_alpha_vantage_api():
    """Teste la connexion √† Alpha Vantage avec votre cl√©"""
    try:
        url = "https://www.alphavantage.co/query"
        params = {
            "function": "GLOBAL_QUOTE",
            "symbol": "AAPL",
            "apikey": ALPHA_VANTAGE_API_KEY
        }
        
        logger.info(f"üîç Test connexion Alpha Vantage avec cl√©: {ALPHA_VANTAGE_API_KEY[:8]}...")
        
        response = requests.get(url, params=params, timeout=10)
        data = response.json()
        
        if "Global Quote" in data:
            price = data['Global Quote']['05. price']
            logger.info(f"‚úÖ Alpha Vantage CONNECT√â! AAPL: ${price}")
            return True
        elif "Note" in data:
            logger.warning(f"‚ö†Ô∏è Note API: {data['Note'][:80]}")
            return True  # API fonctionne mais avec avertissement
        else:
            logger.error(f"‚ùå R√©ponse inattendue: {data}")
            return False
            
    except Exception as e:
        logger.error(f"‚ùå Erreur test API: {e}")
        return False

def fetch_symbol_data(symbol: str, max_days: int = 30):
    """R√©cup√®re les donn√©es d'un symbole depuis Alpha Vantage"""
    try:
        url = "https://www.alphavantage.co/query"
        params = {
            "function": "TIME_SERIES_DAILY",
            "symbol": symbol,
            "apikey": ALPHA_VANTAGE_API_KEY,
            "outputsize": "compact"  # 100 derniers jours
        }
        
        logger.info(f"  üì• {symbol}...")
        
        response = requests.get(url, params=params, timeout=15)
        data = response.json()
        
        # V√©rifier les erreurs
        if "Error Message" in data:
            logger.error(f"    ‚ùå Erreur API: {data['Error Message'][:100]}")
            return []
        
        if "Note" in data:
            logger.warning(f"    ‚ö†Ô∏è Limite API: {data['Note'][:80]}")
            return "RATE_LIMIT"  # Signal sp√©cial pour rate limit
        
        if "Time Series (Daily)" not in data:
            logger.error(f"    ‚ùå Format invalide pour {symbol}")
            return []
        
        # Extraire les donn√©es
        time_series = data["Time Series (Daily)"]
        symbol_data = []
        days_collected = 0
        
        for date_str, values in sorted(time_series.items(), reverse=True):
            if days_collected >= max_days:
                break
            
            symbol_data.append({
                'symbol': symbol,
                'date': date_str,
                'open': float(values["1. open"]),
                'high': float(values["2. high"]),
                'low': float(values["3. low"]),
                'close': float(values["4. close"]),
                'volume': int(values["5. volume"]),
                'dividends': 0.0,
                'stock_splits': 0.0
            })
            days_collected += 1
        
        logger.info(f"    ‚úÖ {symbol}: {days_collected} jours r√©cup√©r√©s")
        return symbol_data
        
    except Exception as e:
        logger.error(f"    ‚ùå Exception pour {symbol}: {str(e)[:100]}")
        return []

def fetch_all_alpha_vantage_data():
    """R√©cup√®re les donn√©es pour tous les symboles avec gestion des limites"""
    logger.info("üöÄ D√âBUT R√âCUP√âRATION ALPHA VANTAGE")
    logger.info(f"Cl√© API: {ALPHA_VANTAGE_API_KEY[:8]}...")
    logger.info(f"Symboles: {len(PROJECT_SYMBOLS)}")
    
    all_data = []
    rate_limit_hit = False
    
    for i, symbol in enumerate(PROJECT_SYMBOLS):
        try:
            # R√©cup√©rer les donn√©es du symbole
            symbol_data = fetch_symbol_data(symbol, max_days=30)
            
            if symbol_data == "RATE_LIMIT":
                rate_limit_hit = True
                logger.warning(f"‚è∏Ô∏è Limite API atteinte apr√®s {i} symboles")
                break
            
            if symbol_data:
                all_data.extend(symbol_data)
            
            # Pause IMPORTANTE entre les appels (5 appels/minute max)
            if i < len(PROJECT_SYMBOLS) - 1 and not rate_limit_hit:
                wait_time = 13  # 13 secondes entre chaque appel
                logger.info(f"‚è≥ Pause {wait_time}s (respect limites API)...")
                time.sleep(wait_time)
                
        except Exception as e:
            logger.error(f"Erreur globale pour {symbol}: {e}")
            continue
    
    # Ajouter les m√©tadonn√©es
    for data_point in all_data:
        data_point.update({
            'batch_id': BATCH_ID,
            'data_source': 'ALPHA_VANTAGE_REAL',
            'api_key_used': ALPHA_VANTAGE_API_KEY[:8] + '...'
        })
    
    logger.info(f"üìä R√âCUP√âRATION TERMIN√âE: {len(all_data)} lignes")
    
    if rate_limit_hit and len(all_data) == 0:
        logger.warning("üîÑ Aucune donn√©e r√©elle, cr√©ation de donn√©es de secours...")
        return create_backup_data()
    
    return all_data

def create_backup_data():
    """Cr√©e des donn√©es r√©alistes en cas d'√©chec API"""
    logger.info("üîÑ Cr√©ation donn√©es de secours r√©alistes...")
    
    all_data = []
    end_date = datetime.now().date()
    
    # Prix r√©alistes bas√©s sur des donn√©es r√©centes
    realistic_prices = {
        'AAPL': 195.50, 'MSFT': 370.25, 'GOOGL': 140.75, 
        'AMZN': 150.30, 'TSLA': 240.80, 'NVDA': 500.60,
        'META': 350.40, 'JNJ': 155.20
    }
    
    for symbol in PROJECT_SYMBOLS:
        base_price = realistic_prices.get(symbol, 100.00)
        
        # G√©n√©rer 30 jours de donn√©es r√©alistes
        for i in range(30):
            date = end_date - timedelta(days=i)
            
            # Variation quotidienne r√©aliste
            daily_change = (hash(f"{symbol}{date}{i}") % 40 - 20) / 1000  # -2% √† +2%
            close_price = round(base_price * (1 + daily_change), 2)
            
            # OHLC coh√©rents
            daily_volatility = close_price * 0.015  # 1.5% de volatilit√©
            open_price = round(close_price * (1 + ((hash(f"{symbol}{i}o") % 10 - 5) / 1000)), 2)
            high_price = round(max(open_price, close_price) + daily_volatility * 0.6, 2)
            low_price = round(min(open_price, close_price) - daily_volatility * 0.4, 2)
            
            # Volume r√©aliste
            base_vol = 10000000  # 10M actions
            volume = int(base_vol * (1 + (hash(f"{symbol}{i}v") % 100) / 100))
            
            all_data.append({
                'symbol': symbol,
                'date': date.strftime('%Y-%m-%d'),
                'open': open_price,
                'high': high_price,
                'low': low_price,
                'close': close_price,
                'volume': volume,
                'dividends': 0.0,
                'stock_splits': 0.0,
                'batch_id': BATCH_ID,
                'data_source': 'BACKUP_SIMULATED',
                'api_key_used': 'BACKUP_DATA'
            })
    
    logger.info(f"üìä Donn√©es secours cr√©√©es: {len(all_data)} lignes")
    return all_data

# ============================================================================
# T√ÇCHES AIRFLOW - CORRIG√âES POUR FINANCE_DB
# ============================================================================

def fetch_financial_data(**context):
    """T√¢che principale de r√©cup√©ration des donn√©es"""
    logger.info("=" * 60)
    logger.info("üéØ PROJET FINANCE - R√âCUP√âRATION DES DONN√âES")
    logger.info(f"√âquipe: Assia Boujnah, Soukaina El Mahjoubi, Khalil Fatima")
    logger.info(f"Batch ID: {BATCH_ID}")
    logger.info(f"Base de donn√©es: {SNOWFLAKE_DATABASE}.{SNOWFLAKE_SCHEMA}")
    logger.info("=" * 60)
    
    # Tester l'API
    api_working = test_alpha_vantage_api()
    
    if api_working:
        logger.info("‚úÖ API Alpha Vantage fonctionnelle, r√©cup√©ration des donn√©es r√©elles...")
        all_data = fetch_all_alpha_vantage_data()
        data_source = all_data[0]['data_source'] if all_data else "NO_DATA"
    else:
        logger.warning("‚ö†Ô∏è API non disponible, donn√©es de secours...")
        all_data = create_backup_data()
        data_source = "BACKUP_DATA"
    
    # Statistiques
    total_rows = len(all_data)
    unique_symbols = len(set([d['symbol'] for d in all_data]))
    unique_dates = len(set([d['date'] for d in all_data]))
    
    logger.info("üìà STATISTIQUES FINALES:")
    logger.info(f"   Source: {data_source}")
    logger.info(f"   Lignes: {total_rows}")
    logger.info(f"   Symboles: {unique_symbols}")
    logger.info(f"   Jours uniques: {unique_dates}")
    if all_data:
        logger.info(f"   P√©riode: {min([d['date'] for d in all_data])} √† {max([d['date'] for d in all_data])}")
    
    # Sauvegarde XCom
    context['ti'].xcom_push(key='financial_data', value=all_data)
    context['ti'].xcom_push(key='batch_id', value=BATCH_ID)
    context['ti'].xcom_push(key='data_source', value=data_source)
    context['ti'].xcom_push(key='stats', value={
        'total_rows': total_rows,
        'unique_symbols': unique_symbols,
        'unique_dates': unique_dates,
        'data_source': data_source
    })
    
    return total_rows

def store_in_snowflake(**context):
    """Stocke les donn√©es dans Snowflake - CORRIG√â pour FINANCE_DB"""
    data = context['ti'].xcom_pull(key='financial_data', task_ids='fetch_financial_data')
    batch_id = context['ti'].xcom_pull(key='batch_id', task_ids='fetch_financial_data')
    data_source = context['ti'].xcom_pull(key='data_source', task_ids='fetch_financial_data')
    
    if not data:
        logger.error("‚ùå Aucune donn√©e √† stocker")
        return 0
    
    logger.info(f"üíæ STOCKAGE SNOWFLAKE")
    logger.info(f"   Database: {SNOWFLAKE_DATABASE}")
    logger.info(f"   Schema: {SNOWFLAKE_SCHEMA}")
    logger.info(f"   Lignes: {len(data)}")
    logger.info(f"   Batch: {batch_id}")
    logger.info(f"   Source: {data_source}")
    
    try:
        # Convertir en DataFrame
        df = pd.DataFrame(data)
        
        # Connexion Snowflake
        hook = SnowflakeHook(snowflake_conn_id='snowflake_conn')
        conn = hook.get_conn()
        
        # V√©rifier/Cr√©er la table dans FINANCE_DB
        cursor = conn.cursor()
        
        # 1. S'assurer qu'on utilise la bonne database
        use_db_sql = f"USE DATABASE {SNOWFLAKE_DATABASE};"
        cursor.execute(use_db_sql)
        
        # 2. Cr√©er le sch√©ma s'il n'existe pas
        create_schema_sql = f"CREATE SCHEMA IF NOT EXISTS {SNOWFLAKE_SCHEMA};"
        cursor.execute(create_schema_sql)
        
        # 3. Cr√©er la table
        create_table_sql = f"""
        CREATE TABLE IF NOT EXISTS {SNOWFLAKE_DATABASE}.{SNOWFLAKE_SCHEMA}.ALPHA_VANTAGE_DATA (
            symbol VARCHAR(10) NOT NULL,
            date DATE NOT NULL,
            open DECIMAL(10, 4),
            high DECIMAL(10, 4),
            low DECIMAL(10, 4),
            close DECIMAL(10, 4),
            volume BIGINT,
            dividends DECIMAL(10, 4),
            stock_splits DECIMAL(10, 4),
            batch_id VARCHAR(50),
            data_source VARCHAR(50),
            api_key_used VARCHAR(20),
            load_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),
            CONSTRAINT pk_alpha_vantage PRIMARY KEY (symbol, date, batch_id)
        )
        COMMENT = 'Donn√©es Alpha Vantage - Projet √âquipe Finance'
        """
        
        cursor.execute(create_table_sql)
        conn.commit()
        logger.info(f"‚úÖ Table cr√©√©e dans {SNOWFLAKE_DATABASE}.{SNOWFLAKE_SCHEMA}")
        
        # Insertion optimis√©e
        from snowflake.connector.pandas_tools import write_pandas
        
        logger.info("üì§ Insertion des donn√©es...")
        
        success, nchunks, nrows, _ = write_pandas(
            conn=conn,
            df=df,
            table_name='ALPHA_VANTAGE_DATA',
            schema=SNOWFLAKE_SCHEMA,
            database=SNOWFLAKE_DATABASE,
            chunk_size=500,
            quote_identifiers=False
        )
        
        if success:
            # Log d√©taill√©
            stats_sql = f"""
            SELECT 
                COUNT(*) as total,
                COUNT(DISTINCT symbol) as symbols,
                MIN(date) as start_date,
                MAX(date) as end_date,
                AVG(close) as avg_price
            FROM {SNOWFLAKE_DATABASE}.{SNOWFLAKE_SCHEMA}.ALPHA_VANTAGE_DATA 
            WHERE batch_id = '{batch_id}'
            """
            
            cursor.execute(stats_sql)
            stats = cursor.fetchone()
            
            logger.info(f"""
            üéâ STOCKAGE R√âUSSI:
            ‚îú‚îÄ Database: {SNOWFLAKE_DATABASE}
            ‚îú‚îÄ Schema: {SNOWFLAKE_SCHEMA}
            ‚îú‚îÄ Lignes ins√©r√©es: {nrows}
            ‚îú‚îÄ Symboles: {stats[1]}
            ‚îú‚îÄ P√©riode: {stats[2]} √† {stats[3]}
            ‚îú‚îÄ Prix moyen: ${stats[4]:.2f}
            ‚îú‚îÄ Source: {data_source}
            ‚îî‚îÄ Batch: {batch_id}
            """)
            
            # Cr√©er/Cr√©er la table de logs
            create_logs_sql = f"""
            CREATE TABLE IF NOT EXISTS {SNOWFLAKE_DATABASE}.{SNOWFLAKE_SCHEMA}.PIPELINE_LOGS (
                log_id INT AUTOINCREMENT,
                pipeline_name VARCHAR(100),
                batch_id VARCHAR(50),
                data_source VARCHAR(50),
                rows_processed INT,
                status VARCHAR(20),
                details VARCHAR(500),
                log_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP()
            )
            """
            cursor.execute(create_logs_sql)
            
            # Ajouter aux logs
            log_sql = f"""
            INSERT INTO {SNOWFLAKE_DATABASE}.{SNOWFLAKE_SCHEMA}.PIPELINE_LOGS 
            (pipeline_name, batch_id, data_source, rows_processed, status, details)
            VALUES (
                'alpha_vantage_pipeline',
                '{batch_id}',
                '{data_source}',
                {nrows},
                'SUCCESS',
                'Symboles: {stats[1]}, P√©riode: {stats[2]} √† {stats[3]}'
            )
            """
            
            cursor.execute(log_sql)
            conn.commit()
            
            cursor.close()
            conn.close()
            
            return nrows
            
        else:
            logger.error("‚ùå √âchec write_pandas")
            raise Exception("√âchec de l'insertion")
            
    except Exception as e:
        logger.error(f"‚ùå Erreur Snowflake: {e}")
        
        # Fallback: insertion SQL manuelle
        try:
            logger.info("üîÑ Tentative insertion SQL manuelle...")
            hook = SnowflakeHook(snowflake_conn_id='snowflake_conn')
            conn = hook.get_conn()
            cursor = conn.cursor()
            
            # Utiliser la bonne database
            cursor.execute(f"USE DATABASE {SNOWFLAKE_DATABASE};")
            cursor.execute(f"USE SCHEMA {SNOWFLAKE_SCHEMA};")
            
            inserted = 0
            for row in data:
                try:
                    insert_sql = f"""
                    INSERT INTO ALPHA_VANTAGE_DATA 
                    (symbol, date, open, high, low, close, volume, dividends, stock_splits, batch_id, data_source, api_key_used)
                    VALUES (
                        '{row['symbol']}', '{row['date']}', {row['open']}, {row['high']}, {row['low']}, 
                        {row['close']}, {row['volume']}, {row['dividends']}, {row['stock_splits']}, 
                        '{batch_id}', '{data_source}', '{row.get('api_key_used', 'N/A')}'
                    )
                    """
                    cursor.execute(insert_sql)
                    inserted += 1
                except Exception as e2:
                    logger.warning(f"Ligne {inserted+1} √©chou√©e: {e2}")
                    continue
            
            conn.commit()
            logger.info(f"‚úÖ Insertion manuelle: {inserted}/{len(data)} lignes")
            
            cursor.close()
            conn.close()
            
            return inserted
            
        except Exception as e2:
            logger.error(f"‚ùå √âchec complet: {e2}")
            raise

def generate_project_report(**context):
    """G√©n√®re un rapport pour le projet"""
    batch_id = context['ti'].xcom_pull(key='batch_id', task_ids='fetch_financial_data')
    stats = context['ti'].xcom_pull(key='stats', task_ids='fetch_financial_data')
    
    logger.info("=" * 60)
    logger.info("üìã RAPPORT FINAL DU PROJET")
    logger.info("=" * 60)
    logger.info(f"√âQUIPE: Assia Boujnah, Soukaina El Mahjoubi, Khalil Fatima")
    logger.info(f"DATE: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"BATCH ID: {batch_id}")
    logger.info(f"DATABASE: {SNOWFLAKE_DATABASE}.{SNOWFLAKE_SCHEMA}")
    logger.info("-" * 60)
    
    if stats:
        logger.info(f"üìä DONN√âES R√âCUP√âR√âES:")
        logger.info(f"   Source: {stats['data_source']}")
        logger.info(f"   Total lignes: {stats['total_rows']}")
        logger.info(f"   Symboles uniques: {stats['unique_symbols']}")
        logger.info(f"   Jours de donn√©es: {stats['unique_dates']}")
    
    logger.info("üîß CONFIGURATION:")
    logger.info(f"   API Alpha Vantage: ‚úÖ Activ√©e")
    logger.info(f"   Cl√© API: {ALPHA_VANTAGE_API_KEY[:8]}...")
    logger.info(f"   Symboles configur√©s: {len(PROJECT_SYMBOLS)}")
    logger.info(f"   Database Snowflake: {SNOWFLAKE_DATABASE}")
    logger.info(f"   Schema Snowflake: {SNOWFLAKE_SCHEMA}")
    
    logger.info("üéØ PROCHAINES √âTAPES:")
    logger.info("   1. V√©rifier les donn√©es dans Snowflake")
    logger.info("   2. Calculer les indicateurs techniques (RSI, MACD, etc.)")
    logger.info("   3. Cr√©er les visualisations Streamlit")
    logger.info("   4. Pr√©parer la comparaison Oozie vs Airflow")
    
    logger.info("=" * 60)
    
    # Sauvegarder le rapport
    report = {
        'team': ['Assia Boujnah', 'Soukaina El Mahjoubi', 'Khalil Fatima'],
        'project': 'Pipeline financier avec Alpha Vantage',
        'database': SNOWFLAKE_DATABASE,
        'schema': SNOWFLAKE_SCHEMA,
        'batch_id': batch_id,
        'execution_time': datetime.now().isoformat(),
        'stats': stats,
        'configuration': {
            'api_used': 'Alpha Vantage',
            'symbols_count': len(PROJECT_SYMBOLS),
            'storage': 'Snowflake',
            'next_steps': ['Technical Indicators', 'Streamlit Dashboard', 'Oozie Comparison']
        }
    }
    
    context['ti'].xcom_push(key='project_report', value=report)
    
    return report

# ============================================================================
# DAG PRINCIPAL - CORRIG√â POUR FINANCE_DB
# ============================================================================

with DAG(
    'alpha_vantage_finance_pipeline',
    default_args=default_args,
    description=f'Pipeline financier avec API Alpha Vantage - Database: {SNOWFLAKE_DATABASE}',
    schedule_interval='0 20 * * 1-5',  # 20h UTC, jours ouvrables
    catchup=False,
    tags=['alpha_vantage', 'finance', 'project', 'equipe', SNOWFLAKE_DATABASE.lower()]
) as dag:

    # T√¢che 1: R√©cup√©ration des donn√©es
    fetch_task = PythonOperator(
        task_id='fetch_financial_data',
        python_callable=fetch_financial_data,
    )
    
    # T√¢che 2: Stockage Snowflake
    store_task = PythonOperator(
        task_id='store_in_snowflake',
        python_callable=store_in_snowflake,
    )
    
    # T√¢che 3: Rapport projet
    report_task = PythonOperator(
        task_id='generate_project_report',
        python_callable=generate_project_report,
    )
    
    # Orchestration
    fetch_task >> store_task >> report_task